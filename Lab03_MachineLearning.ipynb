{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Lab03_MachineLearning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUvdvAaASzmD",
        "colab_type": "text"
      },
      "source": [
        "# Lab03: Machine learning\n",
        "\n",
        "- MSSV: 1712237\n",
        "- Họ và tên: Đặng Tấn Tài"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Esno5cNSzmX",
        "colab_type": "text"
      },
      "source": [
        "## Yêu cầu bài tập\n",
        "\n",
        "**Cách làm bài**\n",
        "\n",
        "\n",
        "Bạn sẽ làm trực tiếp trên file notebook này; trong file, từ `TODO` để cho biết những phần mà bạn cần phải làm.\n",
        "\n",
        "Bạn có thể thảo luận ý tưởng cũng như tham khảo các tài liệu, nhưng *code và bài làm phải là của bạn*. \n",
        "\n",
        "Nếu vi phạm thì sẽ bị 0 điểm cho bài tập này.\n",
        "\n",
        "**Cách nộp bài**\n",
        "\n",
        "Trước khi nộp bài, rerun lại notebook (`Kernel` -> `Restart & Run All`).\n",
        "\n",
        "Sau đó, tạo thư mục có tên `MSSV` của bạn (vd, nếu bạn có MSSV là 1234567 thì bạn đặt tên thư mục là `1234567`) Chép file `Lab03-MachineLearning.ipynb` vào, rồi nén thư mục `MSSV` này lại và nộp ở link trên moodle.\n",
        "\n",
        "**Nội dung bài tập**\n",
        "\n",
        "Bài tập 3 là bài tập cá nhân. Trong bài này, bạn sẽ cài đặt 2 thuật toán học máy: \n",
        "1. Cây quyết định (Decision tree)\n",
        "2. Gaussian Naive Bayes (Lớp CNTN sẽ làm thêm phần 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lYefhgwoSzmZ",
        "colab_type": "text"
      },
      "source": [
        "### Import library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uXTmvF6JSzmb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CO6JKObwSzmn",
        "colab_type": "text"
      },
      "source": [
        "### Load Iris dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6EFp9Jl3Szmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "iris=datasets.load_iris()\n",
        "\n",
        "X=iris.data\n",
        "y=iris.target\n",
        "\n",
        "#split dataset into training data and testing data\n",
        "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz-C6eu2Szmt",
        "colab_type": "text"
      },
      "source": [
        "## 1. Cây quyết định: Iterative Dichotomiser 3 (ID3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEQ7wvnpSzm3",
        "colab_type": "text"
      },
      "source": [
        "### 1.1 Information Gain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9doZ5CKSSzm5",
        "colab_type": "text"
      },
      "source": [
        "Thông tin kỳ vọng (entropy):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmJNW8LxSzm6",
        "colab_type": "text"
      },
      "source": [
        "$$Entropy=-\\sum_{i}^{n}p_ilog_{2}(p_i)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-H-41plSzm7",
        "colab_type": "text"
      },
      "source": [
        "Hàm entropy đạt giá trị nhỏ nhất nếu có một giá trị $p_i=1$, đạt giá trị lớn nhất nếu tất cả các $p_i$ bằng nhau. Những tính chất này của hàm entropy khiến nó được sử dụng trong việc đo độ hỗn loạn của một phép phân chia của ID3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGAwlg1dSzm9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy(counts, n_samples):\n",
        "    \"\"\"\n",
        "    Parameters:\n",
        "    -----------\n",
        "    counts: shape (n_classes): list number of samples in each class\n",
        "    n_samples: number of data samples\n",
        "    \n",
        "    -----------\n",
        "    return entropy \n",
        "    \"\"\"\n",
        "    #TODO\n",
        "    entropy = -np.sum(np.array(counts)/n_samples * np.log2(np.array(counts)/n_samples))\n",
        "    return entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ7on8pvSznN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def entropy_of_one_division(division): \n",
        "    \"\"\"\n",
        "    Returns entropy of a divided group of data\n",
        "    Data may have multiple classes\n",
        "    \"\"\"\n",
        "    n_samples = len(division)\n",
        "    n_classes = set(division)\n",
        "    \n",
        "    counts=[]\n",
        "    #count samples in each class then store it to list counts\n",
        "    #TODO:\n",
        "    for item in n_classes:\n",
        "      counts.append(len(division[division==item]))\n",
        "    \n",
        "    return entropy(counts,n_samples),n_samples\n",
        "\n",
        "\n",
        "def get_entropy(y_predict, y):\n",
        "    \"\"\"\n",
        "    Returns entropy of a split\n",
        "    y_predict is the split decision by cutoff, True/Fasle\n",
        "    \"\"\"\n",
        "    n = len(y)\n",
        "    entropy_true, n_true = entropy_of_one_division(y[y_predict]) # left hand side entropy\n",
        "    entropy_false, n_false = entropy_of_one_division(y[~y_predict]) # right hand side entropy\n",
        "    # overall entropy\n",
        "    #TODO s=?\n",
        "    s = (entropy_true * n_true + entropy_false * n_false) / n\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFI9qoMWSznZ",
        "colab_type": "text"
      },
      "source": [
        "Độ lợi thông tin phân lớp tập D theo thuộc tính A:\n",
        "$$ Gain(A)=Entrophy(D)-Entrophy_{A}(D)$$\n",
        "\n",
        "Trong ID3, tại mỗi node, thuộc tính được chọn được xác định dựa trên là thuộc tính khiến cho information gain đạt giá trị lớn nhất.\n",
        "\n",
        "Các thuộc tính của tập Iris đều có giá trị liên tục. Do đó ta cần rời rạc hóa cho từng thuộc tính. Cách đơn giản là sử dụng một ngưỡng `cutoff` chia giá trị của dữ liệu trên mỗi thuộc tính sẽ làm 2 phần: `<cutoff` và `>=cutoff`.\n",
        "\n",
        "Để tìm ngưỡng `cutoff` tốt nhất cho mỗi thuộc tính ta lần lượt thay `cutoff` bằng các giá trị của thuộc tính sau đó tính entropy, `cutoff` tốt nhất khi entropy bé nhất $ \\left(\\arg\\min Entrophy_{A}(D)\\right)$.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCQvubYDSzna",
        "colab_type": "text"
      },
      "source": [
        "### 1.2 Decision tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npc0RldfSznd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecisionTreeClassifier:\n",
        "    def __init__(self, tree=None, depth=0):\n",
        "        '''Parameters:\n",
        "        -----------------\n",
        "        tree: decision tree\n",
        "        depth: depth of decision tree after training'''\n",
        "        \n",
        "        self.depth = depth\n",
        "        self.tree=tree\n",
        "    def fit(self, X, y, node={}, depth=0):\n",
        "        '''Parameter:\n",
        "        -----------------\n",
        "        X: training data\n",
        "        y: label of training data\n",
        "        ------------------\n",
        "        return: node \n",
        "        \n",
        "        node: each node represented by cutoff value and column index, value and children.\n",
        "         - cutoff value is thresold where you divide your attribute\n",
        "         - column index is your data attribute index\n",
        "         - value of node is mean value of label indexes, \n",
        "           if a node is leaf all data samples will have same label\n",
        "        \n",
        "        Note that: we divide each attribute into 2 part => each node will have 2 children: left, right.\n",
        "        '''\n",
        "        \n",
        "        #Stop conditions\n",
        "        \n",
        "        #if all value of y are the same \n",
        "        if np.all(y==y[0]):\n",
        "            return {'val':y[0]}\n",
        "\n",
        "        else: \n",
        "            col_idx, cutoff, entropy = self.find_best_split_of_all(X, y)    # find one split given an information gain \n",
        "            y_left = y[X[:, col_idx] < cutoff]\n",
        "            y_right = y[X[:, col_idx] >= cutoff]\n",
        "            node = {'index_col':col_idx,\n",
        "                        'cutoff':cutoff,\n",
        "                   'val':np.mean(y)}\n",
        "            node['left'] = self.fit(X[X[:, col_idx] < cutoff], y_left, {}, depth+1)\n",
        "            node['right'] = self.fit(X[X[:, col_idx] >= cutoff], y_right, {}, depth+1)\n",
        "            self.depth += 1 \n",
        "            self.tree = node\n",
        "            return node\n",
        "    \n",
        "    def find_best_split_of_all(self, X, y):\n",
        "        col_idx = None\n",
        "        min_entropy = 1\n",
        "        cutoff = None\n",
        "        for i, col_data in enumerate(X.T):\n",
        "            entropy, cur_cutoff = self.find_best_split(col_data, y)\n",
        "            if entropy == 0:                   #best entropy\n",
        "                return i, cur_cutoff, entropy\n",
        "            elif entropy <= min_entropy:\n",
        "                min_entropy = entropy\n",
        "                col_idx = i\n",
        "                cutoff = cur_cutoff\n",
        "               \n",
        "        return col_idx, cutoff, min_entropy\n",
        "    \n",
        "    def find_best_split(self, col_data, y):\n",
        "        ''' Parameters:\n",
        "        -------------\n",
        "        col_data: data samples in column'''\n",
        "         \n",
        "        min_entropy = 10\n",
        "\n",
        "        #Loop through col_data find cutoff where entropy is minimum\n",
        "        \n",
        "        cutoff = 0\n",
        "\n",
        "        for value in set(col_data):\n",
        "            y_predict = col_data < value\n",
        "            my_entropy = get_entropy(y_predict, y)\n",
        "            #TODO\n",
        "            if (my_entropy < min_entropy):\n",
        "              min_entropy = my_entropy\n",
        "              cutoff = value\n",
        "            #min entropy=?, cutoff=?\n",
        "            \n",
        "        return min_entropy, cutoff\n",
        "                                               \n",
        "    def predict(self, X):\n",
        "        tree = self.tree\n",
        "        pred = np.zeros(shape=len(X))\n",
        "        for i, c in enumerate(X):\n",
        "            pred[i] = self._predict(c)\n",
        "        return pred\n",
        "    \n",
        "    def _predict(self, row):\n",
        "        cur_layer = self.tree\n",
        "        while cur_layer.get('cutoff'):\n",
        "            if row[cur_layer['index_col']] < cur_layer['cutoff']:\n",
        "                cur_layer = cur_layer['left']\n",
        "            else:\n",
        "                cur_layer = cur_layer['right']\n",
        "        else:\n",
        "            return cur_layer.get('val')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3psFlX6gSznt",
        "colab_type": "text"
      },
      "source": [
        "### 1.3 Classification on Iris Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i59zcSdUSznv",
        "colab_type": "code",
        "outputId": "6baf9979-f4cc-4c99-a878-dbc9cacefb60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model = DecisionTreeClassifier()\n",
        "tree = model.fit(X_train, y_train)\n",
        "pred=model.predict(X_train)\n",
        "print('Accuracy of your decision tree model on training data:', accuracy_score(y_train,pred))\n",
        "pred=model.predict(X_test)\n",
        "print('Accuracy of your decision tree model:', accuracy_score(y_test,pred))\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of your decision tree model on training data: 1.0\n",
            "Accuracy of your decision tree model: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjHLZYqNSzn6",
        "colab_type": "text"
      },
      "source": [
        "## 2. Định lý Bayes\n",
        "\n",
        "Định lý Bayes được phát biểu dưới dạng toán học như sau:\n",
        "$$\\begin{equation}\n",
        "P\\left(A|B\\right)= \\dfrac{P\\left(B|A\\right)P\\left(A\\right)}{P\\left(B\\right)}\n",
        "\\end{equation}$$\n",
        "\n",
        "Nếu ta coi $B$ là dữ liệu $\\mathcal{D}$, các thông số cần ước tính $A$ là $w$, ta có:\n",
        "\n",
        "$$ \\begin{align}\n",
        "    \\underbrace{P(w|\\mathcal{D})}_{Posterior}= \\dfrac{1}{\\underbrace{P(\\mathcal{D})}_{Normalization}} \\overbrace{P(\\mathcal{D}|w)}^{\\text{Likelihood}} \\overbrace{P(w)}^{Prior}\n",
        "    \\end{align}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRTTxdhtSzn8",
        "colab_type": "text"
      },
      "source": [
        "#### Naive Bayes\n",
        "Để giúp cho việc tính toán được đơn giản, người ta thường giả sử một cách đơn giản nhất rằng các thành phần của biến ngẫu nhiên $D$ (hay các thuộc tính của dữ liệu $D$) là độc lập với nhau, nếu biết $w$. Tức là:\n",
        "$$P(\\mathcal{D}|w)=\\prod _{i=1}^{d}P(x_i|w)$$\n",
        "\n",
        "$d$: số lượng thuộc tính\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BqXEPzMSzn9",
        "colab_type": "text"
      },
      "source": [
        "### 2.1. Probability Density Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40JhBXHsSzn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pdf:\n",
        "    def __init__(self,hist=None):\n",
        "        '''\n",
        "        A probability density function represented by a histogram\n",
        "        \n",
        "        hist: shape (n,1), n: number of hypotheses\n",
        "        hypo: hypothesis (simply understand as label)\n",
        "        ------------------\n",
        "        hist[hypo]=P(hypo)\n",
        "        '''\n",
        "        self.hist = hist\n",
        "        \n",
        "    #virtual function\n",
        "    def likelihood(self, data, hypo):\n",
        "        '''Paramters:\n",
        "        data: new data record \n",
        "        hypo: hypothesis (simply understand as label)\n",
        "        ---------\n",
        "        return P(data/hypo)\n",
        "        ''' \n",
        "        raise Exception()\n",
        "            \n",
        "    #update histogram for new data \n",
        "    def update(self, data):\n",
        "        ''' \n",
        "        P(hypo/data)=P(data/hypo)*P(hypo)*(1/P(data))\n",
        "        '''\n",
        "        \n",
        "        #Likelihood * Prior \n",
        "        #TODO\n",
        "        for hypo in self.hist.keys():\n",
        "            #self.hist[hypo]=?\n",
        "            self.hist[hypo] = self.hist[hypo] * self.likelihood(data, hypo)\n",
        "            \n",
        "        #Normalization\n",
        "        \n",
        "        #TODO: s=P(data)\n",
        "        #s=?\n",
        "\n",
        "        s = sum(self.hist.values())\n",
        "        \n",
        "        for hypo in self.hist.keys():\n",
        "            self.hist[hypo] = self.hist[hypo]/s\n",
        "        \n",
        "    def plot_pdf(self):\n",
        "        #plot Histogram\n",
        "        #TODO\n",
        "        X = np.arange(len(self.hist))\n",
        "        plt.bar(X, self.hist.values(), align='center', width=0.5)\n",
        "        plt.xticks(X, self.hist.keys())\n",
        "        plt.show()\n",
        "        return\n",
        "    def maxHypo(self):\n",
        "        #find the hypothesis with maximum probability from hist\n",
        "        #TODO\n",
        "        v = list(self.hist.values())\n",
        "        k = list(self.hist.keys())\n",
        "        return k[v.index(max(v))]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UImyM0ttSzoJ",
        "colab_type": "text"
      },
      "source": [
        "### 2.2 Classification on Iris Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lonlXQdWSzoL",
        "colab_type": "text"
      },
      "source": [
        "#### Gaussian Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vD0ZmqplSzoM",
        "colab_type": "text"
      },
      "source": [
        "- Naive Bayes có thể được mở rộng cho dữ liệu với các thuộc tính có giá trị là số thực, phổ biến nhất bằng cách sử dụng phân phối chuẩn (Gaussian distribution).\n",
        "\n",
        "- Phần mở rộng này được gọi là Gaussian Naive Bayes. Các hàm khác có thể được sử dụng để ước tính phân phối dữ liệu, nhưng Gauss (hoặc phân phối chuẩn) là dễ nhất để làm việc vì chỉ cần ước tính giá trị trung bình và độ lệch chuẩn từ dữ liệu huấn luyện."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ax4E7_deSzoN",
        "colab_type": "text"
      },
      "source": [
        "#### Define Gauss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOjpgllESzoO",
        "colab_type": "text"
      },
      "source": [
        "$$ f\\left(x;\\mu,\\sigma \\right)= \\dfrac{1}{\\sigma \\sqrt{2\\pi}} \n",
        "\\exp \\left({-\\dfrac{\\left(x-\\mu\\right)^2}{2 \\sigma^2}}\\right) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTHe4UlgSzoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Gauss(std,mean,x):\n",
        "    #Calculate the Gaussian probability distribution function for x\n",
        "    #TODO \n",
        "    result = 1/(std*np.sqrt(2*np.pi)) * np.exp(-(x - mean)*(x - mean)/(2*std*std))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmjU1ynKSzoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NBGaussian(pdf):\n",
        "    def __init__(self, hist=None, std=None, mean=None):\n",
        "        '''Parameters:\n",
        "        \n",
        "        '''\n",
        "        pdf.__init__(self, hist)\n",
        "        self.std=std\n",
        "        self.mean=mean\n",
        "    def likelihood(self,data, hypo):\n",
        "        '''\n",
        "        Returns: res=P(data/hypo)\n",
        "        -----------------\n",
        "        Naive bayes:\n",
        "            Atributes are assumed to be conditionally independent given the class value.\n",
        "        '''\n",
        "    \n",
        "        std=self.std[hypo]\n",
        "        mean=self.mean[hypo]\n",
        "        res=1\n",
        "        #TODO\n",
        "        #res=res*P(x1/hypo)*P(x2/hypo)...\n",
        "        for i in range(len(data)):\n",
        "          res = res * Gauss(std[i], mean[i], data[i])  \n",
        "        return res \n",
        "    def fit(self, X,y):\n",
        "        \"\"\"Parameters:\n",
        "        X: training data\n",
        "        y: labels of training data\n",
        "        \"\"\"\n",
        "        n=len(X)\n",
        "        #number of iris species\n",
        "        #TODO\n",
        "        #n_species=???\n",
        "        n_species = len(set(y))\n",
        "        \n",
        "        hist={}\n",
        "        mean={}\n",
        "        std={}\n",
        "        \n",
        "        #separate  dataset into rows by class\n",
        "        for hypo in range(0,n_species):\n",
        "            #rows have hypo label\n",
        "            #TODO rows=?\n",
        "            rows = []\n",
        "            for i in range(n):\n",
        "              if (y[i] == hypo):\n",
        "                rows.append(X[i])\n",
        "            #histogram for each hypo\n",
        "            #TODO probability=?\n",
        "            probability = len(rows)/n\n",
        "            hist[hypo]=probability\n",
        "            \n",
        "            #Each hypothesis represented by its mean and standard derivation\n",
        "            '''mean and standard derivation should be calculated for each column (or each attribute)'''\n",
        "            #TODO mean[hypo]=?, std[hypo]=?\n",
        "            mean[hypo] = np.mean(rows, axis=0)\n",
        "            std[hypo] = np.std(rows, axis=0)\n",
        "            \n",
        "         \n",
        "        self.mean=mean\n",
        "        self.std=std\n",
        "        self.hist=hist\n",
        "   \n",
        "    def _predict(self, data, plot=False):\n",
        "        \"\"\"\n",
        "        Predict label for only 1 data sample\n",
        "        ------------\n",
        "        Parameters:\n",
        "        data: data sample\n",
        "        plot: True: draw histogram after update new data sample\n",
        "        -----------\n",
        "        return: label of data\n",
        "        \"\"\"\n",
        "        model=NBGaussian(hist=self.hist.copy(),std=self.std.copy(), mean=self.mean.copy())\n",
        "        model.update(data)\n",
        "        if (plot): model.plot_pdf()\n",
        "        return model.maxHypo()\n",
        "    \n",
        "    def predict(self, data):\n",
        "        \"\"\"Parameters:\n",
        "        Data: test data\n",
        "        ----------\n",
        "        return labels of test data\"\"\"\n",
        "        \n",
        "        pred=[]\n",
        "        for x in data:\n",
        "            pred.append(self._predict(x))\n",
        "        return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eKUIQBqSzoZ",
        "colab_type": "text"
      },
      "source": [
        "#### Show histogram of training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzxeLe-WSzoa",
        "colab_type": "code",
        "outputId": "e71a6710-cd7c-440c-bd11-d26ec7f86d54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "model_1=NBGaussian()\n",
        "model_1.fit(X_train, y_train)\n",
        "model_1.plot_pdf()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAD+uYVa0brV7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY3riuZHSzoe",
        "colab_type": "text"
      },
      "source": [
        "#### Test wih 1 data record"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY_sMQSYSzoe",
        "colab_type": "code",
        "outputId": "971a8bae-c50f-4bb3-fc07-57fb5a86ee34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "#label of y_test[10]\n",
        "print('Label of X_test[10]: ', y_test[10])\n",
        "#update model and show histogram with X_test[10]:\n",
        "\n",
        "print('Our histogram after update X_test[10]: ')\n",
        "model_1._predict(X_test[10],plot=True)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Label of X_test[10]:  2\n",
            "Our histogram after update X_test[10]: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAALbklEQVR4nO3cf6jd913H8edryap/rE4wVyj5sQTM\nxDCFjksdFLS4DZIKiaBIA/MXZfnHymRDiChV4j/OwQQh/giszA1tjVPkwiL5QysFWUtut1mWhIxL\nnObGQbOuVsfQGHj7xz2Vs9t7c75JvjfHvPN8wIXz/X4/nPMOp33y5XvO+aaqkCTd+9427wEkSeMw\n6JLUhEGXpCYMuiQ1YdAlqYnt83rhHTt21N69e+f18pJ0T3r55Ze/UVULGx2bW9D37t3L8vLyvF5e\nku5JSf5ls2NecpGkJgy6JDVh0CWpCYMuSU0YdElqYmbQkzyT5NUkX9nkeJL8QZKVJK8kee/4Y0qS\nZhlyhv5p4OBNjh8C9k/+jgF/dOdjSZJu1cygV9ULwDdvsuQI8Jla8yLwvUkeGmtASdIwY1xD3wlc\nmdpeneyTJN1Fd/WXokmOsXZZhj179tzNl5b0/9Te45+f9wh33dd+9ye35HnHOEO/Cuye2t412fcW\nVXWqqharanFhYcNbEUiSbtMYQV8Cfn7ybZf3AW9U1ddHeF5J0i2YecklybPAY8COJKvAbwFvB6iq\nPwbOAI8DK8C3gV/aqmElSZubGfSqOjrjeAG/PNpEkqTb4i9FJakJgy5JTRh0SWrCoEtSEwZdkpow\n6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0Y\ndElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYM\nuiQ1YdAlqYlBQU9yMMmlJCtJjm9wfE+S55N8KckrSR4ff1RJ0s3MDHqSbcBJ4BBwADia5MC6Zb8J\nnK6qh4EngD8ce1BJ0s0NOUN/BFipqstVdR14Djiybk0B3zN5/E7g38YbUZI0xJCg7wSuTG2vTvZN\n+23gQ0lWgTPAr2z0REmOJVlOsnzt2rXbGFeStJmxPhQ9Cny6qnYBjwOfTfKW566qU1W1WFWLCwsL\nI720JAmGBf0qsHtqe9dk37QngdMAVfUF4LuBHWMMKEkaZkjQzwH7k+xL8gBrH3ourVvzr8D7AZL8\nEGtB95qKJN1FM4NeVTeAp4CzwEXWvs1yPsmJJIcnyz4GfDjJPwHPAr9YVbVVQ0uS3mr7kEVVdYa1\nDzun9z099fgC8Oi4o0mSboW/FJWkJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6Qm\nDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1IT\nBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1MSgoCc5mORSkpUkxzdZ\n87NJLiQ5n+TPxx1TkjTL9lkLkmwDTgIfBFaBc0mWqurC1Jr9wK8Dj1bV60m+f6sGliRtbMgZ+iPA\nSlVdrqrrwHPAkXVrPgycrKrXAarq1XHHlCTNMiToO4ErU9urk33T3g28O8k/JnkxycGxBpQkDTPz\nksstPM9+4DFgF/BCkh+uqn+fXpTkGHAMYM+ePSO9tCQJhp2hXwV2T23vmuybtgosVdX/VNU/A19l\nLfDfoapOVdViVS0uLCzc7sySpA0MCfo5YH+SfUkeAJ4Altat+RvWzs5JsoO1SzCXR5xTkjTDzKBX\n1Q3gKeAscBE4XVXnk5xIcniy7CzwWpILwPPAr1XVa1s1tCTprQZdQ6+qM8CZdfuennpcwEcnf5Kk\nOfCXopLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMu\nSU0YdElqwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGX\npCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktTEoKAnOZjkUpKVJMdvsu6nk1SSxfFGlCQN\nMTPoSbYBJ4FDwAHgaJIDG6x7EPgI8NLYQ0qSZhtyhv4IsFJVl6vqOvAccGSDdb8DfBz4rxHnkyQN\nNCToO4ErU9urk33/J8l7gd1V9fmbPVGSY0mWkyxfu3btloeVJG3ujj8UTfI24JPAx2atrapTVbVY\nVYsLCwt3+tKSpClDgn4V2D21vWuy700PAu8B/iHJ14D3AUt+MCpJd9eQoJ8D9ifZl+QB4Alg6c2D\nVfVGVe2oqr1VtRd4EThcVctbMrEkaUMzg15VN4CngLPAReB0VZ1PciLJ4a0eUJI0zPYhi6rqDHBm\n3b6nN1n72J2PJUm6Vf5SVJKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLU\nhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElq\nwqBLUhMGXZKaMOiS1IRBl6QmDLokNWHQJakJgy5JTRh0SWrCoEtSE4OCnuRgkktJVpIc3+D4R5Nc\nSPJKkr9L8q7xR5Uk3czMoCfZBpwEDgEHgKNJDqxb9iVgsap+BPgc8HtjDypJurkhZ+iPACtVdbmq\nrgPPAUemF1TV81X17cnmi8CucceUJM0yJOg7gStT26uTfZt5EvjbjQ4kOZZkOcnytWvXhk8pSZpp\n1A9Fk3wIWAQ+sdHxqjpVVYtVtbiwsDDmS0vSfW/7gDVXgd1T27sm+75Dkg8AvwH8eFX99zjjSZKG\nGnKGfg7Yn2RfkgeAJ4Cl6QVJHgb+BDhcVa+OP6YkaZaZQa+qG8BTwFngInC6qs4nOZHk8GTZJ4B3\nAH+Z5MtJljZ5OknSFhlyyYWqOgOcWbfv6anHHxh5LknSLfKXopLUhEGXpCYMuiQ1YdAlqQmDLklN\nGHRJasKgS1ITBl2SmjDoktSEQZekJgy6JDVh0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6Qm\nDLokNWHQJakJgy5JTRh0SWrCoEtSEwZdkpow6JLUhEGXpCYMuiQ1YdAlqQmDLklNGHRJasKgS1IT\nBl2SmjDoktTEoKAnOZjkUpKVJMc3OP5dSf5icvylJHvHHlSSdHMzg55kG3ASOAQcAI4mObBu2ZPA\n61X1A8DvAx8fe1BJ0s0NOUN/BFipqstVdR14Djiybs0R4E8njz8HvD9JxhtTkjTL9gFrdgJXprZX\ngR/dbE1V3UjyBvB9wDemFyU5BhybbH4ryaXbGXrOdrDu36V2fI/vD3N7n3Nn1zDetdmBIUEfTVWd\nAk7dzdccW5Llqlqc9xzaOr7H94eO7/OQSy5Xgd1T27sm+zZck2Q78E7gtTEGlCQNMyTo54D9SfYl\neQB4Alhat2YJ+IXJ458B/r6qarwxJUmzzLzkMrkm/hRwFtgGPFNV55OcAJaragn4FPDZJCvAN1mL\nflf39CUjDeJ7fH9o9z7HE2lJ6sFfikpSEwZdkpow6APNuv2B7n1JnknyapKvzHsWbZ0ku5M8n+RC\nkvNJPjLvmcbiNfQBJrc/+CrwQdZ+WHUOOFpVF+Y6mEaV5MeAbwGfqar3zHsebY0kDwEPVdUXkzwI\nvAz8VIf/nz1DH2bI7Q90j6uqF1j7lpYaq6qvV9UXJ4//E7jI2q/d73kGfZiNbn/Q4j8A6X42uTPs\nw8BL851kHAZd0n0pyTuAvwJ+tar+Y97zjMGgDzPk9geS7hFJ3s5azP+sqv563vOMxaAPM+T2B5Lu\nAZNbe38KuFhVn5z3PGMy6ANU1Q3gzdsfXAROV9X5+U6lsSV5FvgC8INJVpM8Oe+ZtCUeBX4O+Ikk\nX578PT7vocbg1xYlqQnP0CWpCYMuSU0YdElqwqBLUhMGXZKaMOiS1IRBl6Qm/her7INR542yAAAA\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mf7iK9bMSzoh",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate your Gaussian Naive Bayes model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGPqkiRoSzok",
        "colab_type": "code",
        "outputId": "8ea4075e-a965-41e9-d913-fa6012c43ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pred=model_1.predict(X_test)\n",
        "print('Accuracy of your Gaussian Naive Bayes model:', accuracy_score(y_test,pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of your Gaussian Naive Bayes model: 0.96\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}